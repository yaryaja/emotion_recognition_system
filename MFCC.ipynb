{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Import the audio playback widget\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import lfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing an audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LABELS :-\n",
    " 0 - ANGER\n",
    " 1 - BORED\n",
    " 2 - DISGUST\n",
    " 3 - ANXIETY\n",
    " 4 - HAPPY\n",
    " 5 - SAD\n",
    " 6 - Neutral\n",
    "\n",
    "GENDER :-\n",
    "  1 - MALE\n",
    "  0 - FEMALE\n",
    "'''\n",
    "labels_encoded = {'W':0, 'L':1, 'E':2, 'A':3, 'F':4, 'T':5, 'N':6}\n",
    "gender_encoded = {'03':1 , '10' : 1, '11':1 , '12':1 , '15':1, '08':0, '09':0, '13':0 , '14':0, '16':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "file_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './wav'\n",
    "for file_name in os.listdir(dataset_path):\n",
    "  if file_name.endswith('.wav'):\n",
    "    file_paths.append(file_name)\n",
    "    labels.append(labels_encoded[file_name[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'file_path':file_paths,\n",
    "        'label':labels,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = np.array(df.label.unique())\n",
    "plot_x.sort()\n",
    "emotion_unique_vals = df.label.value_counts(sort = True)\n",
    "plot_y = []\n",
    "for x in plot_x:\n",
    "  plot_y.append(emotion_unique_vals[x])\n",
    "emotion_unique_vals = pd.DataFrame(emotion_unique_vals)\n",
    "# plt.bar( plot_x, plot_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file_path']\n",
    "df_2_audio_vals = []\n",
    "x = 1\n",
    "for audio_file in df['file_path']:\n",
    "  data, sampling_rate = librosa.load(f'./wav/{audio_file}')\n",
    "  data_mfcc = librosa.feature.mfcc(y = data , sr = sampling_rate, n_mfcc = 13)\n",
    "  df_2_audio_vals.append(data_mfcc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding maximum windows present in all the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_length = [aud.shape[1] for aud in df_2_audio_vals]\n",
    "max_col_len = max(columns_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making everything to same length\n",
    "df_2_audio_vals_padded = df_2_audio_vals.copy()\n",
    "for ind, arr in enumerate(df_2_audio_vals_padded):\n",
    "  arr_sh = arr.shape[1]\n",
    "  zero_cols = max_col_len - arr_sh\n",
    "  zero_arr = np.zeros((arr.shape[0], zero_cols))\n",
    "  res_arr = np.hstack((arr, zero_arr))\n",
    "  df_2_audio_vals_padded[ind] = res_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "for ind in range(0, len(df_2_audio_vals_padded)):\n",
    "  df_2_audio_vals_padded[ind] = scaler.fit_transform(df_2_audio_vals_padded[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame(\n",
    "    {\n",
    "        'audio': df_2_audio_vals_padded,\n",
    "        'labels': labels\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FLATTEN THE MATRIX IN EACH CELL\n",
    "df_2_flattened = df_2.copy()\n",
    "df_2_flattened['audio'] = df_2_flattened['audio'].apply(lambda x : x.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2_flattened['audio'].values\n",
    "y = df_2['labels'].values\n",
    "X_array =[]\n",
    "y_array=[]\n",
    "for i in range(len(X)):\n",
    "    temp=X[i].tolist()\n",
    "    X_array.append(temp)\n",
    "    y_array.append(df_2['labels'][i])\n",
    "\n",
    "X_array = np.array(X_array)\n",
    "y_array = np.array(y_array)\n",
    "X = X_array\n",
    "y = y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5092592592592593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,svm_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5092592592592593"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "model=RandomForestClassifier()\n",
    "# model.seed(10)\n",
    "model.fit(X_train,Y_train)\n",
    "model.score(X_test,Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
